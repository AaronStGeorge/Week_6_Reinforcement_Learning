{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "In the following two exercises, we will be using Q-Learning to solve two simple reinforcement learning problems. Each exercise has sections with ##TO DO## comments that you will have to change to make the code run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Armed Bandit Problem\n",
    "Imagine you are sitting infront of a slot machine with 10 buttons. When you press each button, you get a random reward that is generated from an unknown probability distribution. Each button has a different probability distribution with different means and variances. This means that given enough time, selecting the button with the highest mean will maximize your reward. However, you don't know which button this is. Below, you will implement an $\\epsilon$-greedy approach to maximize your reward at this slot machine.\n",
    "## $\\epsilon$ (epsilon)-greedy algorithm\n",
    "\"One very famous approach to solving reinforcement learning problems is the $\\epsilon$ (epsilon)-greedy algorithm, such that, with a probability $\\epsilon$, you will choose an action a at random (exploration), and the rest of the time (probability 1âˆ’$\\epsilon$) you will select the best lever based on what you currently know from past plays (exploitation). So most of the time you play greedy, but sometimes you take some risks and choose a random lever and see what happens.\"\n",
    "\n",
    "Experiment inspired by https://www.datacamp.com/community/tutorials/introduction-reinforcement-learning#implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the reward from the choice, mean and variance in the Q_values table\n",
    "def Reward(choice, buttons_mean, buttons_variance):\n",
    "    current_reward = np.random.normal(buttons_mean[choice],buttons_variance[choice])\n",
    "    return current_reward\n",
    "\n",
    "#Trial creates a new Q table to enact the Policy over \"it\" number of iterations\n",
    "def Trial(eps,num_buttons,buttons_mean,buttons_variance,it=1000):\n",
    "    Q_values = np.zeros(num_buttons) #initialize Q_values table\n",
    "    Q_tracker = np.zeros(num_buttons) #initialze tracking table\n",
    "    all_rewards = []\n",
    "    for i in range(it):\n",
    "        choice = Policy(eps,num_buttons,Q_values) #make choice\n",
    "        current_reward = Reward(choice,buttons_mean,buttons_variance) #get reward\n",
    "        all_rewards.append(current_reward) #store reward\n",
    "        Q_values,Q_tracker = UpdateQTable(choice,current_reward,Q_values,Q_tracker) #calculate new mean in Q_values table\n",
    "    return all_rewards #return rewards from the trials\n",
    "\n",
    "#Experiment now takes a list of epsilons to determine how different choices of greedy epsilons affect convergence of the Policy\n",
    "def Experiment(eps_list,num_buttons):\n",
    "    eps_rewards = []\n",
    "    for eps in eps_list: #iterate over different values of epsilon and run a trial for each\n",
    "        eps_rewards.append(Trial(eps,num_buttons,buttons_mean,buttons_variance))        \n",
    "    return eps_rewards #return all rewards for each epsilon value and trial\n",
    "    \n",
    "\n",
    "'''TO DO SECTION'''\n",
    "    \n",
    "#create an epsilon-greedy policy function that will allow our n-armed bandit to choose which button to press\n",
    "def Policy(eps,num_buttons,Q_values):\n",
    "    #initialize a random greedy probability\n",
    "    greedy_prob =  ###TO DO###\n",
    "   \n",
    "    #make a random choice if the greedy_prob is less than eps otherwise take the best choice\n",
    "    ###TO DO ###\n",
    "    \n",
    "    return choice    \n",
    "\n",
    "#write a function to figure out how to update the Q_values table\n",
    "#UpdateQTable updates both Q_values with a new current mean and also updates Q_tracker, tracking the number of times\n",
    "#Q_tracker[choice] has been chosen\n",
    "#return Q_value, Q_tracker as a tuple\n",
    "def UpdateQTable(choice,current_reward,Q_values,Q_tracker):\n",
    "    ###TO DO###\n",
    "    return Q_values, Q_tracker   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_list = [] ###TO DO### choose 3 values of epsilon\n",
    "\n",
    "num_buttons = 10 #number of buttons or arms our machine has\n",
    "\n",
    "buttons_mean = [np.random.normal(100,10) for x in range(num_buttons)] #initialize mean of each button\n",
    "buttons_variance = [np.random.normal(5,2) for x in range(num_buttons)] #initialize variance of each button\n",
    "\n",
    "# run the experiment\n",
    "final_exp = ###TO DO###\n",
    "\n",
    "#compute running average of reward\n",
    "for i in range(len(final_exp)):\n",
    "    for j in range(len(final_exp[0])):\n",
    "        final_exp[i][j] = np.mean(final_exp[i][:j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the convergence of the reward's running average for various epsilon values\n",
    "#if you use more or less than 3 epsilons you will need to change the code below\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(final_exp[0],'b',label = 'First Epsilon')#blue\n",
    "plt.plot(final_exp[1],'r',label = 'Second Epsilon')#red\n",
    "plt.plot(final_exp[2],'k',label = 'Third Epsilon')#black\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The self-driving taxi problem\n",
    "In this exercise, we are going to use Q-Learning to teach a self-driving taxi how to most efficiently pick up and drop off passengers. Imagine that the parking lot pictured below is our environment and the taxi is our agent: \n",
    "\n",
    "![alt text](https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png \"Taxi Example\")\n",
    "\n",
    "In this environment, there are 5 rows and 5 columns that the taxi can move between. Additionally, there are four cells labelled R, G, Y, B that represent the locations passengers can be picked up and dropped off at. Additionally, there are some barriers that the taxi cannot drive through. In the above example, the passenger is located in pickup location Y and would like to be dropped off at location R. \n",
    "\n",
    "There are a variety of states that our environment can be in at a given timestep. There are 5 rows the taxi can be located in, 5 columns the taxi can be located in, five locations the passenger can be located (the four locations plus the taxi), and four locations the passenger can be dropped off. This gives the environment a state space of size 500 (5 * 5 * 5 * 4). The action space of the environment is much simpler, with only 6 options:\n",
    "\n",
    "1. Move south (0)\n",
    "2. Move north (1)\n",
    "3. Move east (2)\n",
    "4. Move west (3)\n",
    "5. Pickup passenger (4)\n",
    "6. Dropoff passenger (5)\n",
    "\n",
    "The 'rules' for the environment are as follows: \n",
    "- Every timestep, the agent gets -1 point.\n",
    "- The agent gets +20 points if it successfully drops off a passenger in the correct location.\n",
    "- The agent gets -10 points if it tries to pick up a passenger in an incorrect cell.\n",
    "- The agent gets -10 points if it tries to drop off a passenger at the incorrect cell. \n",
    "\n",
    "Using this information, we will use reinforcement Q-learning to teach our agent how to most efficiently pick up and drop off passengers.\n",
    "\n",
    "You will have to change all of the ##TO DO## comments to make the code run properly.\n",
    "\n",
    "This exercise borrows some of the code and examples from this [blog post](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a classes to help us keep track of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "# Will be used to keep track of the environment's state\n",
    "State = namedtuple('State', ['row', 'col', 'passenger', 'destination'])\n",
    "\n",
    "# Class to display and update environment state. Keeps track of points and whether or not problem is solved.\n",
    "class Environment(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initialize a random state\n",
    "        self.row, self.col, self.passenger, self.destination = self.reset()\n",
    "        self.state = State(##TO DO##) # Hint: Look at the namedtuple above to see what we want to store in our state.\n",
    "        \n",
    "        # Define walls and pickup/dropoff locations\n",
    "        self.walls = [(0, 1), (3, 0), (4, 0), (3, 2), (4, 2)]\n",
    "            \n",
    "        # Dictionary that will map the passenger's state to a location on the map\n",
    "        self.locs = [(0, 0), (0, 4), (4, 0), (4, 3), (-1, -1)]\n",
    "        self.loc_dict = dict(zip([0, 1, 2, 3, 4], self.locs))\n",
    "        \n",
    "        # Dictionary containing all possible states that will map a state to a row in the Q-table\n",
    "        self.state_space = self.populate_state_space()\n",
    "\n",
    "    \n",
    "    # Some inspiration from: https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py\n",
    "    def step(self, action):\n",
    "        \n",
    "        penalties = 0\n",
    "        points = -1\n",
    "        complete = False\n",
    "        col, row, passenger = self.col, self.row, self.passenger\n",
    "\n",
    "        if action == 0: # Step south\n",
    "            row = min(self.row + 1, 4)\n",
    "            \n",
    "        elif action == 1: # Step north\n",
    "            row = max(self.row - 1, 0)\n",
    "            \n",
    "        elif action == 2 and (self.row, self.col) not in self.walls: # Step east\n",
    "            col = min(self.col + 1, 4)\n",
    "            \n",
    "        elif action == 3 and (self.row, self.col - 1) not in self.walls: # Step west\n",
    "            col = max(self.col - 1, 0)\n",
    "            \n",
    "        elif action == 4: # Pickup passenger\n",
    "            if self.passenger != 4 and (self.row, self.col) == self.loc_dict[self.passenger]:\n",
    "                passenger = 4\n",
    "            else:\n",
    "                points += -10\n",
    "                penalties += 1\n",
    "                \n",
    "        elif action == 5: # Dropoff passenger\n",
    "            \n",
    "            # If holding the passenger and in the destination, the task is complete.\n",
    "            if self.passenger == 4 and (self.row, self.col) == self.loc_dict[self.destination]:\n",
    "                points += 20\n",
    "                complete = True\n",
    "            else:\n",
    "                points += -10\n",
    "                penalties += 1\n",
    "        \n",
    "        # Update location, points and state\n",
    "        self.row, self.col, self.passenger = ## TO DO ##\n",
    "        self.points += points\n",
    "        self.penalties += penalties\n",
    "        self.complete = complete\n",
    "        self.state = ## TO DO ##\n",
    "        \n",
    "        return self.state, points, complete\n",
    "        \n",
    "    def populate_state_space(self):\n",
    "        \n",
    "        space = []\n",
    "        \n",
    "        # Iterate through all possible state spaces to make dictionary that can be used as to create the Q-table\n",
    "        for row in range(5):    \n",
    "            for col in range(5):\n",
    "                for passenger in range(5):\n",
    "                    for destination in range(4):\n",
    "                        space.append(State(row=row, col=col, passenger=passenger, destination=destination))\n",
    "                        \n",
    "        return dict(zip(space, range(len(space))))\n",
    "        \n",
    "    \n",
    "    # Reset to initial random state\n",
    "    def reset(self):\n",
    "        row = np.random.randint(5)\n",
    "        col = np.random.randint(5)\n",
    "        # initialize this to 4 instead of 5 so passenger doesn't start in taxi\n",
    "        passenger = np.random.randint(4)\n",
    "        destination = np.random.randint(4)\n",
    "        \n",
    "        # Make sure passenger and destination aren't the same\n",
    "        while passenger == destination:\n",
    "            passenger = np.random.randint(4)\n",
    "            destination = np.random.randint(4)\n",
    "        \n",
    "        self.points = 0\n",
    "        self.penalties = 0\n",
    "        self.complete = False\n",
    "        self.row, self.col, self.passenger, self.destination = row, col, passenger, destination\n",
    "\n",
    "        return row, col, passenger, destination\n",
    "    \n",
    "    # Print out environment\n",
    "    def display(self, action=None):\n",
    "        loc_map = {self.loc_dict[self.passenger]: 'P',\n",
    "                   self.loc_dict[self.destination]: 'D',\n",
    "                   (self.row, self.col): 'T'}\n",
    "        \n",
    "        out = '+---------+\\n'\n",
    "        for row in range(5):\n",
    "            out += '|'\n",
    "            for col in range(5):\n",
    "\n",
    "                if (row, col) in loc_map:\n",
    "                    out += loc_map[(row, col)]\n",
    "                else: \n",
    "                    out += ' '\n",
    "                if (row, col) in self.walls or col == 4:\n",
    "                    out += '|'\n",
    "                else:\n",
    "                    out += ':'\n",
    "            out += '\\n'\n",
    "                \n",
    "        out += '+---------+\\n'\n",
    "        out += 'Points: {}\\n'.format(self.points)\n",
    "        \n",
    "        if action is not None:\n",
    "            \n",
    "            act_map = {0: 'Moved South',\n",
    "                       1: 'Moved North',\n",
    "                       2: 'Moved East',\n",
    "                       3: 'Moved West',\n",
    "                       4: 'Picked Up Passenger',\n",
    "                       5: 'Dropped Off Passenger'}\n",
    "            \n",
    "            out += 'Action: {}\\n'.format(act_map[action])\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(self.display())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets see how well we do by choosing random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Initialize environment\n",
    "env = Environment()\n",
    "\n",
    "# Lists to store points\n",
    "points = []\n",
    "eps = []\n",
    "errors = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    \n",
    "    frames = []\n",
    "    epochs = 0\n",
    "    \n",
    "    # Repeat while the task is not complete\n",
    "    while ## TO DO ##: Hint: There is an attribute in the environment class that says whether or not the task is complete. \n",
    "        \n",
    "        # Take a random action\n",
    "        action = np.random.randint(6)\n",
    "        \n",
    "        ## TO DO ## Replace with method in class that takes a step for us\n",
    "        frames.append(env.display(action) + 'Epochs: {}\\n'.format(epochs))\n",
    "\n",
    "        epochs += 1\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print('{} Epochs completed'.format(i))\n",
    "    points.append(env.points)\n",
    "    errors.append(env.penalties)\n",
    "    eps.append(epochs)\n",
    "    env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print out solution\n",
    "for frame in frames:\n",
    "    clear_output(wait=True)\n",
    "    print(frame)\n",
    "    sleep(.01)\n",
    "        \n",
    "plt.hist(points)\n",
    "plt.title('Distribution of Scores When Solving Problem Randomly')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(eps)\n",
    "plt.title('Distribution of Timesteps Required to Solve Problem Randomly')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(errors)\n",
    "plt.title('Distribution of Errors When Solving Problem Randomly')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, selecting actions at random isn't a good way to solve this problem. It can take up to 1000 epochs to find the passenger, pick them up, and drop them off at the correct location. Next, lets try implementing a Q-Learning function to see if it helps speed up the process. First, we will have to learn a Q function that maps state, action pairs to a reward. We can define this Q function as:\n",
    "\n",
    "$$\n",
    "Q(s, a) = (1-\\alpha)Q(s, a) + \\alpha(r + \\gamma \\max\\limits_{a} Q(s', a))\n",
    "$$\n",
    "\n",
    "where $a$ is the action we take, $s$ and $s'$ are the current and next state, respectively, r is the reward,  $\\alpha$ is the step size, and $\\gamma$ is the discount on future rewards. Recall that we also need an $\\epsilon$ hyperparameter that defines the probability with which we take a random action. This parameter will ensure that we both explore the entire state space and take actions that maximize reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Q-table that is of size (state space by action space)\n",
    "q_table = np.zeros([500, 6])\n",
    "\n",
    "env = Environment()\n",
    "\n",
    "# Map each state to a unique id\n",
    "state_map = env.state_space\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "rewards = []\n",
    "\n",
    "for i in range(1, 50001):\n",
    "    \n",
    "    state = ## TO DO ## Replace with Environment method that resets to a new state\n",
    "    \n",
    "    # Will keep track of metrics as model trains\n",
    "    epochs, penalties, points = 0, 0, 0\n",
    "    \n",
    "    while not env.complete:\n",
    "        \n",
    "        # Map the action to a location in the Q-table\n",
    "        state_num = state_map[state]\n",
    "        \n",
    "        # Take random action with probability epsilon\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.randint(6)\n",
    "        \n",
    "        # Otherwise take the action that maximizes reward\n",
    "        else:\n",
    "            action = ## TO DO ## Replace with action that maximizes reward for the current state\n",
    "\n",
    "        # Determine the state and reward given by the step taken by an action \n",
    "        next_state, reward, _ = ## TO DO ## \n",
    "        \n",
    "        # Map the action to a location in the Q-table\n",
    "        next_num = state_map[next_state]\n",
    "        \n",
    "        # Look up the reward in the Q-table for the last time step\n",
    "        old_value = ## TO DO ##\n",
    "        \n",
    "        # Predict the maximum possible reward for the next timestep\n",
    "        next_max = ## TO DO ##\n",
    "        \n",
    "        # Update Q-Table with discounted future reward. This is where we implement the equation given above.\n",
    "        # In this case, our Q 'function' is the Q-Table\n",
    "        new_value = ## TO DO ##\n",
    "        q_table[state_num, action] = new_value\n",
    "        \n",
    "        # Reset the old state to the next state\n",
    "        state = ## TO DO ## \n",
    "        epochs += 1\n",
    "    \n",
    "    rewards.append(env.points)\n",
    "    all_penalties.append(env.penalties)\n",
    "    all_epochs.append(epochs)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print('{} Epochs completed'.format(i))\n",
    "\n",
    "plt.plot(range(50000), rewards)\n",
    "plt.title('Reward as model trains')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(50000), all_penalties)\n",
    "plt.title('Incorrect pickup/dropoff occurences')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(50000), all_epochs)\n",
    "plt.title('Time required to solve problem')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "eps = []\n",
    "errors = []\n",
    "\n",
    "all_frames = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    \n",
    "    state = ## TODO ## Reinitialize the environment\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    frames = []\n",
    "    while not env.complete:\n",
    "        \n",
    "        state_num = state_map[state]\n",
    "\n",
    "        # Always take the action that maximizes reward\n",
    "        action = ## TO DO ##\n",
    "            \n",
    "        state, reward, _ = ## TO DO ## Take a step according to the action\n",
    "\n",
    "        epochs += 1\n",
    "        frames.append(env.display(action))\n",
    "        \n",
    "    all_frames.append(frames)\n",
    "    \n",
    "    points.append(env.points)\n",
    "    errors.append(env.penalties)\n",
    "    eps.append(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a solution\n",
    "for frame in all_frames[-1]:\n",
    "    clear_output(wait=True)\n",
    "    print(frame)\n",
    "    sleep(0.5)\n",
    "\n",
    "plt.hist(points)\n",
    "plt.title('Trained Score Distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(eps)\n",
    "plt.title('Trained Epoch Distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(errors)\n",
    "plt.title('Trained Mistake Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env)\n",
    "print(q_table[state_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions: \n",
    "1. Why do some indices other than the dropoff index (index 5) in the q_table have high rewards in the final state (printed above)? If we are only getting positive points for dropping off the passenger in this state, shouldn't all other rewards be negative?\n",
    "2. What specifically are each of the three hyperparameters ($\\alpha$, $\\gamma$ and $\\epsilon$) accomplishing?\n",
    "3. Is the trained model being as efficient as possible? Is it taking any incorrect steps after training?\n",
    "4. If you move some of the walls in the environment by changing the env.walls attribute, will the trained agent still be able to accomplish the task? Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
